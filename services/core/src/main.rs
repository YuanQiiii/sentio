use anyhow::Result;
use shared_logic::config;
use sentio_llm::{DeepSeekClient, LlmClient, LlmRequest};

#[tokio::main]
async fn main() -> Result<()> {
    // ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–å…¨å±€é…ç½®
    config::initialize_config().await?;

    // ç¬¬äºŒæ­¥ï¼šåŸºäºé…ç½®åˆå§‹åŒ–é¥æµ‹ç³»ç»Ÿ
    let global_config = config::get_config();
    sentio_telemetry::init_telemetry_with_config(&global_config.telemetry)?;

    // ç¬¬ä¸‰æ­¥ï¼šæ‰“å°å¯åŠ¨æ—¥å¿—
    tracing::info!(
        log_level = ?global_config.telemetry.log_level,
        database_url = %global_config.database.url,
        llm_provider = %global_config.llm.provider,
        server_host = %global_config.server.host,
        server_port = %global_config.server.port,
        "Configuration loaded successfully. System starting."
    );

    // å±•ç¤ºä¸€äº›é…ç½®åŠ è½½çš„è¯¦ç»†ä¿¡æ¯
    tracing::debug!(
        smtp_host = %global_config.email.smtp.host,
        "Email configuration loaded"
    );

    tracing::debug!(
        model = %global_config.llm.model,
        timeout = %global_config.llm.timeout,
        max_retries = %global_config.llm.max_retries,
        "LLM configuration loaded"
    );

    // æ¼”ç¤ºåœ¨ç¨‹åºå…¶ä»–åœ°æ–¹å¦‚ä½•è®¿é—®å…¨å±€é…ç½®
    demonstrate_global_config_access();

    // æ¼”ç¤º LLM æœåŠ¡é›†æˆ
    if let Err(e) = demonstrate_llm_integration().await {
        tracing::warn!(
            error = %e,
            "LLM demonstration failed (this is expected if API key is not configured)"
        );
    }

    // ç¨‹åºæ­£å¸¸é€€å‡º
    tracing::info!("System shutdown completed.");
    Ok(())
}

/// æ¼”ç¤ºå¦‚ä½•åœ¨åº”ç”¨çš„ä»»ä½•åœ°æ–¹è®¿é—®å…¨å±€é…ç½®
fn demonstrate_global_config_access() {
    let config = config::get_config();
    tracing::info!(
        "Demonstrating global config access - Database max connections: {}",
        config.database.max_connections
    );
}

/// æ¼”ç¤º LLM æœåŠ¡é›†æˆ
async fn demonstrate_llm_integration() -> Result<()> {
    tracing::info!("Initializing LLM client...");
    
    // åˆ›å»º LLM å®¢æˆ·ç«¯
    let llm_client = DeepSeekClient::new()?;
    
    // åˆ›å»ºç¤ºä¾‹è¯·æ±‚
    let request = LlmRequest::new(
        "ä½ æ˜¯ Sentio AI é‚®ä»¶åŠ©æ‰‹ï¼Œä¸€ä¸ªä¸“ä¸šã€å‹å¥½çš„æ™ºèƒ½é‚®ä»¶ä¼™ä¼´ã€‚".to_string(),
        "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ï¼Œè¯´æ˜ä½ çš„ä¸»è¦åŠŸèƒ½å’Œç‰¹ç‚¹ã€‚".to_string(),
    );
    
    tracing::info!(
        request_id = %request.id,
        "Sending demo request to LLM"
    );
    
    // å‘é€è¯·æ±‚ï¼ˆä»…åœ¨æœ‰æ•ˆ API å¯†é’¥æ—¶æ‰§è¡Œï¼‰
    let response = llm_client.generate_response(&request).await?;
    
    tracing::info!(
        request_id = %response.request_id,
        tokens_used = response.token_usage.total_tokens,
        latency_ms = response.metadata.latency_ms,
        "LLM response received successfully"
    );
    
    // è¾“å‡ºå“åº”å†…å®¹ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­è¿™åº”è¯¥ä¿å­˜åˆ°æ—¥å¿—æˆ–è¿”å›ç»™è°ƒç”¨è€…ï¼‰
    println!("\nğŸ¤– Sentio AI å›å¤:");
    println!("{}", response.content);
    println!();
    
    Ok(())
}
